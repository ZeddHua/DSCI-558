{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "sized-wallace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-13T05:12:31.966428Z",
     "iopub.status.busy": "2022-09-13T05:12:31.966428Z",
     "iopub.status.idle": "2022-09-13T05:12:31.984380Z",
     "shell.execute_reply": "2022-09-13T05:12:31.984380Z",
     "shell.execute_reply.started": "2022-09-13T05:12:31.966428Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "with open('./spider_imdb/spider_imdb/spiders/Zhenmin_Hua_hw01_director.jsonlines') as f:\n",
    "    corpus = []\n",
    "    for i in f.readlines():\n",
    "        corpus.append(eval(i)['biography'])\n",
    "\n",
    "lst = []\n",
    "for biog in corpus:\n",
    "    doc = nlp(biog)\n",
    "    \n",
    "    # awards, lexical\n",
    "    patterns = [\n",
    "        [{'IS_TITLE': True}, {'ORTH': 'nomination'}],\n",
    "        [{'IS_TITLE': True}, {'ORTH': 'nominations'}],\n",
    "        [{'ORTH': 'Golden'}, {'ORTH': 'Lion'}],\n",
    "        [{'ORTH': 'Lifetime'}, {'ORTH': 'Achievement'}],\n",
    "        [{'ORTH': 'Emmy'}],\n",
    "        [{'ORTH': 'Oscar'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'Award'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'Awards'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    awards_lexical = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        awards_lexical.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        awards_lexical = list(set(awards_lexical))\n",
    "    except:\n",
    "        awards_lexical = 'null'\n",
    "\n",
    "    # awards, syntactic\n",
    "    patterns = [\n",
    "        [{'IS_TITLE': True, 'POS': 'PROPN'}, {'ORTH': 'nomination'}],\n",
    "        [{'IS_TITLE': True, 'POS': 'PROPN'}, {'ORTH': 'nominations'}],\n",
    "        [{'ORTH': 'Golden'}, {'ORTH': 'Lion'}],\n",
    "        [{'ORTH': 'Lifetime'}, {'ORTH': 'Achievement'}],\n",
    "        [{'ORTH': 'Emmy'}],\n",
    "        [{'ORTH': 'Oscar'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+', 'POS': 'PROPN'}, {'ORTH': 'Award'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+', 'POS': 'PROPN'}, {'ORTH': 'Awards'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    awards_syntactic = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        awards_syntactic.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        awards_syntactic = list(set(awards_syntactic))\n",
    "    except:\n",
    "        awards_syntactic = 'null'\n",
    "\n",
    "    # actors, lexical\n",
    "    patterns = [\n",
    "        [{'IS_TITLE': True, 'OP': '+'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    actors_lexical = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        actors_lexical.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        actors_lexical = list(set(actors_lexical))\n",
    "    except:\n",
    "        actors_lexical = 'null'\n",
    "\n",
    "    # actors, syntactic\n",
    "    patterns = [\n",
    "        [{'ENT_TYPE': 'PERSON', 'OP': '+'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    actors_syntactic = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        actors_syntactic.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        actors_syntactic = list(set(actors_syntactic))\n",
    "    except:\n",
    "        actors_syntactic = 'null'\n",
    "\n",
    "    # education, lexical\n",
    "    patterns = [\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'College'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'University'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'School'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    ed_lexical = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        ed_lexical.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        education_lexical = ed_lexical[-1]\n",
    "    except:\n",
    "        education_lexical = 'null'\n",
    "\n",
    "    # education, syntactic\n",
    "    patterns = [\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'College', 'POS': 'PROPN'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'University', 'POS': 'PROPN'}],\n",
    "        [{'IS_TITLE': True, 'OP': '+'}, {'ORTH': 'School', 'POS': 'PROPN'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    ed_syntactic = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        ed_syntactic.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        education_syntactic = ed_lexical[-1]\n",
    "    except:\n",
    "        education_syntactic = 'null'\n",
    "\n",
    "    # movies, syntactic\n",
    "    patterns = [\n",
    "        [{'IS_TITLE': True, 'OP':'+'}, {'IS_SPACE': True}, {'ORTH': '('}, {'ENT_TYPE': 'DATE'}, {'ORTH': ')'}],\n",
    "        [{'IS_TITLE': True, 'OP':'+'}, {'IS_LOWER': True}, {'IS_TITLE': True, 'OP':'+'}, {'IS_SPACE': True}, {'ORTH': '('}, {'ENT_TYPE': 'DATE'}, {'ORTH': ')'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    movies_syntactic = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        movies_syntactic.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        movies_syntactic = [i[:-8] for i in movies_syntactic]\n",
    "    except:\n",
    "        movies_syntactic = 'null'\n",
    "\n",
    "    # movies, lexical\n",
    "    patterns = [\n",
    "        [{'IS_TITLE': True, 'OP':'+'}, {'IS_SPACE': True}, {'ORTH': '('}, {'LIKE_NUM': True}, {'ORTH': ')'}],\n",
    "        [{'IS_TITLE': True, 'OP':'+'}, {'IS_LOWER': True}, {'IS_TITLE': True, 'OP':'+'}, {'IS_SPACE': True}, {'ORTH': '('}, {'LIKE_NUM': True}, {'ORTH': ')'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    movies_lexical = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        movies_lexical.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        movies_lexical = [i[:-8] for i in movies_lexical]\n",
    "    except:\n",
    "        movies_lexical = 'null'\n",
    "\n",
    "    # birthplace, lexical\n",
    "    patterns = [\n",
    "        [{'ORTH': 'born'}, {'ORTH': 'in'}, {'IS_TITLE': True}, {'IS_PUNCT': True, 'OP':'*'}, {'IS_TITLE': True, 'OP':'*'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    birth_lexical = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        birth_lexical.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        birthplace_lexical = birth_lexical[-1][8:]\n",
    "    except:\n",
    "        birthplace_lexical = 'null'\n",
    "\n",
    "    # birthplace, syntactic\n",
    "    patterns = [\n",
    "        [{'ORTH': 'born'}, {'ORTH': 'in'}, {'ENT_TYPE': 'GPE'}, {'IS_PUNCT': True, 'OP':'*'}, {'ENT_TYPE': 'GPE', 'OP':'*'}]\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", patterns, greedy='LONGEST') \n",
    "    matches = matcher(doc)\n",
    "\n",
    "    birth_syntactic = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        birth_syntactic.append(span.text)\n",
    "        #print(span.text)\n",
    "\n",
    "    try:\n",
    "        birthplace_syntactic = birth_syntactic[-1][8:]\n",
    "    except:\n",
    "        birthplace_syntactic = 'null'\n",
    "        \n",
    "    #\n",
    "    dic = {}\n",
    "    dic['education_lexical'] = education_lexical\n",
    "    dic['education_syntactic'] = education_syntactic\n",
    "    dic['birthplace_lexical'] = birthplace_lexical\n",
    "    dic['birthplace_syntactic'] = birthplace_syntactic\n",
    "    dic['movies_lexical'] = movies_lexical\n",
    "    dic['movies_syntactic'] = movies_syntactic\n",
    "    dic['awards_lexical'] = awards_lexical\n",
    "    dic['awards_syntactic'] = awards_syntactic\n",
    "    dic['actors_lexical'] = actors_lexical\n",
    "    dic['actors_syntactic'] = actors_syntactic\n",
    "    lst.append(dic)\n",
    "\n",
    "# into jsonlines\n",
    "with open('Zhenmin_Hua_hw01_directorsIE.jsonlines', 'w') as out:\n",
    "    for ddict in lst:\n",
    "        jout = json.dumps(ddict) + '\\n'\n",
    "        out.write(jout)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
