{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuQvvpzl1KFn"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change torch.__version__ of following installation command.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1BuufsZW1RdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ],
      "metadata": {
        "id": "e-XDBJWC1NKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "zzmywOKX64q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "#dataset = TUDataset(root='/tmp/imdb', name='IMDB-BINARY')\n",
        "\n",
        "### add code for using the node degree as input features.\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "\n",
        "class NormalizedDegree(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        deg = degree(data.edge_index[0], dtype=torch.float)\n",
        "        deg = (deg - self.mean) / self.std\n",
        "        data.x = deg.view(-1, 1)\n",
        "        return data\n",
        "\n",
        "\n",
        "def get_dataset(name, sparse=True, cleaned=False):\n",
        "    # path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    # dataset = TUDataset(path, name, cleaned=cleaned)\n",
        "    dataset = TUDataset(root='/tmp/imdb', name=name)\n",
        "    dataset.data.edge_attr = None\n",
        "\n",
        "    if dataset.data.x is None:\n",
        "        max_degree = 0\n",
        "        degs = []\n",
        "        for data in dataset:\n",
        "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
        "            max_degree = max(max_degree, degs[-1].max().item())\n",
        "\n",
        "        if max_degree < 1000:\n",
        "            dataset.transform = T.OneHotDegree(max_degree)\n",
        "        else:\n",
        "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
        "            mean, std = deg.mean().item(), deg.std().item()\n",
        "            dataset.transform = NormalizedDegree(mean, std)\n",
        "\n",
        "    if not sparse:\n",
        "        num_nodes = max_num_nodes = 0\n",
        "        for data in dataset:\n",
        "            num_nodes += data.num_nodes\n",
        "            max_num_nodes = max(data.num_nodes, max_num_nodes)\n",
        "\n",
        "        # Filter out a few really large graphs in order to apply DiffPool.\n",
        "        if name == 'REDDIT-BINARY':\n",
        "            num_nodes = min(int(num_nodes / len(dataset) * 1.5), max_num_nodes)\n",
        "        else:\n",
        "            num_nodes = min(int(num_nodes / len(dataset) * 5), max_num_nodes)\n",
        "\n",
        "        indices = []\n",
        "        for i, data in enumerate(dataset):\n",
        "            if data.num_nodes <= num_nodes:\n",
        "                indices.append(i)\n",
        "        dataset = dataset.copy(torch.tensor(indices))\n",
        "\n",
        "        if dataset.transform is None:\n",
        "            dataset.transform = T.ToDense(num_nodes)\n",
        "        else:\n",
        "            dataset.transform = T.Compose(\n",
        "                [dataset.transform, T.ToDense(num_nodes)])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset = get_dataset('IMDB-BINARY')\n",
        "### add train, val, test loader.\n",
        "train_dataset = dataset[:800]\n",
        "val_dataset = dataset[800:900]\n",
        "test_dataset = dataset[900:]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "qa71S04A69Tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fbd10c-5265-413a-9043-d42816cdf6cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/IMDB-BINARY.zip\n",
            "Extracting /tmp/imdb/IMDB-BINARY/IMDB-BINARY.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXKm1sovmD_X",
        "outputId": "584e108c-a0a8-4d30-d139-a1fef380aa15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IMDB-BINARY(1000)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU4qNONNY3ag",
        "outputId": "a10c182c-a066-4cbe-ac23-3f2dc9f92703"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch_geometric.loader.dataloader.DataLoader at 0x7f0b77ba5790>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UvNkMMqaQDm",
        "outputId": "7ec01f74-16e5-4cbd-8638-abd9d0176d0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf4TC8pCaUcE",
        "outputId": "f614bfc7-6800-4848-f673-60209589a2da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.num_node_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcXT0f1SaYEn",
        "outputId": "4bfd93df-f373-4366-ef5b-5a3b41809bc1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "T9o5Cmdg_JOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_add_pool, global_mean_pool\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
        "        self.conv2 = GCNConv(32, 8)\n",
        "        self.lin1 = Linear(8, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        ### You can add more layers or alter the model structure. See geometric documents which layer or model you can use.\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        \n",
        "        ### aggregate node embeddings into one representation\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "\n",
        "        ### Pass aggregated representation to linear layer to make final prediction\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.lin1(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "g9owzc4n_Ivc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train function"
      ],
      "metadata": {
        "id": "NvWzPpdO1tUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, data.y.view(-1))\n",
        "        loss.backward()\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_dataset)"
      ],
      "metadata": {
        "id": "4VsB7RDR1XPm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation function"
      ],
      "metadata": {
        "id": "1O7At9Td18N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def val(loader):\n",
        "    model.eval()\n",
        "    loss_all = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, data.y.view(-1))\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "    return loss_all / len(val_dataset)"
      ],
      "metadata": {
        "id": "PSIBQfM017Hc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test function"
      ],
      "metadata": {
        "id": "1K7uh53LAQ85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data)\n",
        "        pred = output.max(dim=1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "1STWHVtmAR3e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main code"
      ],
      "metadata": {
        "id": "qFpnv-5U1-dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN().to(device)\n",
        "#data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "\n",
        "number_of_epochs = 200 # You can change.\n",
        "\n",
        "tmp = 100\n",
        "best_model = model\n",
        "best_epoch = 0\n",
        "for epoch in range(number_of_epochs):\n",
        "    train_loss = train()\n",
        "    val_loss = val(val_loader)\n",
        "\n",
        "    # Choose the lowest validation loss checkpoint (you can implement early stopping as well)\n",
        "    if val_loss < tmp:\n",
        "      tmp = val_loss\n",
        "      best_model = model \n",
        "      best_epoch = epoch\n",
        "    #print(val_loss)   \n",
        "print(f'Best_epoch: {best_epoch}, min_val_score: {tmp}')\n",
        "\n",
        "# Load the lowest validation loss checkpoint and check the performance.\n",
        "model = best_model\n",
        "test_acc = test(test_loader)\n",
        "print('Performance: ', test_acc)"
      ],
      "metadata": {
        "id": "Kqw5zHpyAYC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dec9968-831f-497d-8bfc-8059c1ee2033"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best_epoch: 73, min_val_score: 0.48040027141571046\n",
            "Performance:  0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ymh2qsAT17iU"
      }
    }
  ]
}